{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests as re\n",
    "from lxml import html\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data relate to climbing difficult grades\n",
    "data = sqlite3.connect(\"dataset/database.sqlite\")\n",
    "data.row_factory = sqlite3.Row\n",
    "query = data.execute(\"SELECT * From grade\")\n",
    "row = query.fetchone()\n",
    "names = row.keys()\n",
    "grade_results = pd.DataFrame.from_records(data = query.fetchall(), columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data relate to the climbing ascents\n",
    "data = sqlite3.connect(\"dataset/database.sqlite\")\n",
    "data.row_factory = sqlite3.Row\n",
    "query = data.execute(\"SELECT * From ascent\")\n",
    "row = query.fetchone()\n",
    "names = row.keys()\n",
    "ascents_results = pd.DataFrame.from_records(data = query.fetchall(), columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some infos from the dataset\n",
    "ascents_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the columns\n",
    "ascents_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the nameless rows\n",
    "df = ascents_results[ascents_results['name'].str.contains('\\?',regex=True)]\n",
    "ascents_results.drop(df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that will not be used\n",
    "drop_cols = ['user_id','total_score', 'date', 'year', 'last_year', 'rec_date','project_ascent_date','crag_id','sector_id', 'sector','comment', 'rating', 'description', 'yellow_id', 'climb_try','repeat', 'exclude_from_ranking', 'user_recommended', 'chipped','raw_notes','method_id','notes']\n",
    "ascents_results.drop(drop_cols, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove repeated rows by name\n",
    "ascents_results.drop_duplicates('name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a DataFrame for web scrapping the name of location where the ascents were made\n",
    "crags = ascents_results['crag'].unique()\n",
    "crags = pd.DataFrame(crags,columns=['crags'])\n",
    "crags.insert(1,'city','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all the string in lower case for web scrapping\n",
    "crags = crags.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate name os crags\n",
    "crags.drop_duplicates(subset='crags',keep='last', inplace=True)\n",
    "crags = crags.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scrapping of locations in a French climbing site\n",
    "url = 'https://climbingaway.fr/en/results?search='\n",
    "for crag in crags['crags']:\n",
    "    lista = {}\n",
    "    lista['crag'] = crag\n",
    "    page = re.get(str(url+urllib.parse.quote(crag)))\n",
    "    r = html.fromstring(page.content)\n",
    "    try:\n",
    "        lista['location'] = r.xpath('/html/body/div[1]/div[2]/div/div/div[1]/div[1]/div[1]/span/text()[1]')\n",
    "    except:\n",
    "        lista['location'] = ''\n",
    "    listas.append(lista)\n",
    "\n",
    "city_result = pd.DataFrame(listas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and remove the elements which the location were not found by web scrapping\n",
    "empty = city_result[city_result['location']=='[]']\n",
    "city_result.drop(empty.index, inplace=True)\n",
    "city_result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the string returnerd by web scrapping is a entire location from country to city, or province, thisfor will split and extract the city or province\n",
    "res_list = []\n",
    "for i in city_result['location']:\n",
    "    try:\n",
    "        aux = i.split(', ')[2:4]\n",
    "    except:\n",
    "        aux =[]\n",
    "    res_list.append(aux)\n",
    "city_result['city'] = np.array(res_list,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some strings does not bring an entire location, so the columns city remains empty due the try and except executed before. This will clean the empty results\n",
    "aux = city_result[city_result['city'] == '[]']\n",
    "city_result.drop(aux.index, inplace=True)\n",
    "city_result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear and remove special characters from the location and city strings\n",
    "city_result['location'] = city_result['location'].replace(\"'\",'', regex=True)\n",
    "city_result['location'] = city_result['location'].replace(\"\\[\",'', regex=True)\n",
    "city_result['location'] = city_result['location'].replace(\"\\]\",'', regex=True)\n",
    "city_result['location'] = city_result['location'].replace('\"','', regex=True)\n",
    "city_result['city'] = city_result['city'].replace(\"'\",'', regex=True)\n",
    "city_result['city'] = city_result['city'].replace(\"\\[\",'', regex=True)\n",
    "city_result['city'] = city_result['city'].replace(\"\\]\",'', regex=True)\n",
    "city_result['city'] = city_result['city'].replace('\"','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latitude and longitude using GeoPy and city columns\n",
    "aux = []\n",
    "for i in df['city']:\n",
    "    info = {}\n",
    "    info['city'] = i\n",
    "    try:\n",
    "        local = Nominatim(user_agent='climb_study').geocode(i)\n",
    "        info['lat'] = local.latitude\n",
    "        info['long']  = local.longitude\n",
    "    except:\n",
    "        info['lat'] =''\n",
    "        info['long'] = ''\n",
    "    aux.append(info)\n",
    "location = pd.DataFrame(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latitude and longitude using GeoPy and location columns, to obtain tha maximum quantify of locations\n",
    "aux = []\n",
    "for i in df['location']:\n",
    "    info = {}\n",
    "    info['location'] = i\n",
    "    try:\n",
    "        local = Nominatim(user_agent='climb_study').geocode(i)\n",
    "        info['lat'] = local.latitude\n",
    "        info['long']  = local.longitude\n",
    "    except:\n",
    "        info['lat'] =''\n",
    "        info['long'] = ''\n",
    "    aux.append(info)\n",
    "    \n",
    "location2 = pd.DataFrame(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear and remove duplicate cities and location\n",
    "location.drop_duplicates(subset='city',inplace=True)\n",
    "location.reset_index()\n",
    "location2.drop_duplicates(subset='location',inplace=True)\n",
    "location2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframe os citys with lat and long with the original dataset os ascents\n",
    "result_cities = pd.merge(city_result,location, how='left', left_on='city',right_on='city')\n",
    "result_locations = pd.merge(city_result,location2, how='left', left_on='location',right_on='location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create auxiliar dataframes to drop the rows with null values\n",
    "aux_city = result_cities[result_cities['lat'].isnull()==True]\n",
    "aux_location = result_locations[result_locations['lat'].isnull()==True]\n",
    "\n",
    "result_cities.drop(aux_city.index,inplace=True)\n",
    "result_locations.drop(aux_location.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append result dataframes\n",
    "crag_location = result_cities.append(result_locations)\n",
    "crag_location.drop_duplicates(subset='crag',inplace=True)\n",
    "crag_location.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the result of location and fill new columns of original ascents results\n",
    "final_results = pd.merge(ascents_results,crag_location, how='left', left_on='crag',right_on='crag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with a list of all the result which does not match with any crag in dataframe of location\n",
    "empty_route = route_location[route_location['lat'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the empty rows\n",
    "final_ascents_results = final_results.drop(empty_route.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove in column and reset index\n",
    "final_ascents_results.drop(columns='id',inplace=True)\n",
    "final_ascents_results.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unecessary columns\n",
    "grade_drop=['score','fra_routes_input','fra_routes_selector','fra_boulders','fra_boulders_input','fra_boulders_selector','usa_routes','usa_routes_input','usa_routes_selector','usa_boulders_input','usa_boulders_selector']\n",
    "grade.drop(grade_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the ascent dataframe with the grade dataframes\n",
    "final_ascents_results = pd.merge(final_results,grade, how='left', left_on='grade_id',right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an auxiliar dataframe to filter the grade from different types of climbing\n",
    "lista = []\n",
    "count=0\n",
    "for i in final_results['climb_type']:\n",
    "    grade ={}\n",
    "    if i==0:\n",
    "        grade['grade_route'] = final_results['fra_routes'][count]\n",
    "        grade['grade_boulder'] = ''\n",
    "    else:\n",
    "        grade['grade_route'] = ''\n",
    "        grade['grade_boulder'] = final_results['usa_boulders'][count]\n",
    "    lista.append(grade)\n",
    "    count+=1\n",
    "aux = pd.DataFrame(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the new dataframe with the final dataframe\n",
    "final_ascents_results = pd.concat([final_ascents_results, aux.reindex(final_results.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first columns of grade without filter\n",
    "final_ascents_results.drop(columns=['fra_routes','usa_boulders','id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe\n",
    "final_ascents_results.to_csv('final_climbing_dataset.csv',encoding='utf-16',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting columns to remove from grade_results\n",
    "drop = ['score','fra_routes_input','fra_routes_selector','fra_boulders','fra_boulders_input','fra_boulders_selector','usa_routes','usa_routes_input','usa_routes_selector','usa_boulders_input','usa_boulders_selector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the columns from the dataframe\n",
    "grade_results.drop(drop,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataset\n",
    "grade_results.to_csv('final_grades_results.csv',encoding='utf-16',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
